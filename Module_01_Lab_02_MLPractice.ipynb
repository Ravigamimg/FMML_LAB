{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravigamimg/FMML_LAB/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0eff60-a991-4430-8c9c-1d7273fc5f72"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4846d545-160f-427f-d5ed-73967ae46891"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82cf9121-9276-4295-eba8-b27c7bada692"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcfa639-56af-4da1-d11f-392e532e4d96"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cd8a8f-dc56-434f-f576-2d8b9df6f9e5"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03392e6a-42fe-4711-fbfa-20703cbf89a0"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Increase Validation Set Percentage:\n",
        "        When you increase the percentage of data in the validation set, you're giving the model more data to evaluate itself on.\n",
        "        This can lead to a more reliable estimate of how well the model generalizes to unseen data.\n",
        "        The accuracy on the validation set might become more stable and closer to the model's true performance.\n",
        "\n",
        "    Decrease Validation Set Percentage:\n",
        "        When you decrease the percentage of data in the validation set, you're using less data to evaluate the model.\n",
        "        This can make the accuracy on the validation set less reliable because it's based on a smaller sample.\n",
        "        The accuracy may fluctuate more from one evaluation to another, and it might not accurately represent how well the model performs on new, unseen data."
      ],
      "metadata": {
        "id": "ZsAkZy7HFm2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Larger Train Set:\n",
        "        If you have a larger training set, it means the model learns from more data.\n",
        "        This can lead to a better understanding of the underlying patterns in the data, potentially resulting in a more accurate model.\n",
        "        With a larger training set, the model has a better chance of generalizing well to the test set, so the validation set's accuracy is a better indicator of what to expect on the test set.\n",
        "\n",
        "    Larger Validation Set:\n",
        "        A larger validation set gives you a more reliable estimate of how well the model generalizes to unseen data.\n",
        "        It can provide a better assessment of the model's performance, making your predictions about test set accuracy more accurate.\n",
        "        However, it also means you have less data for training, which might affect the model's learning quality.\n",
        "\n",
        "    Balance Between Train and Validation Set:\n",
        "        Finding the right balance between the size of the training and validation sets is crucial.\n",
        "        If the training set is too small, the model may not learn enough, and the validation set's accuracy may not predict test set performance accurately."
      ],
      "metadata": {
        "id": "d2M0AzOjFu21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple words, finding a good percentage to reserve for the validation set to balance the two factors (model learning and accurate evaluation) depends on your specific dataset and the problem you're solving. However, a common rule of thumb is to use around 20% to 30% of your data for the validation set.\n",
        "\n",
        "Here's a bit more detail:\n",
        "\n",
        "- If you allocate too much data to the validation set (e.g., more than 30%), you might not have enough data left for the model to learn effectively during training. This can lead to an underfit model that doesn't capture the data's patterns well.\n",
        "\n",
        "- On the other hand, if you allocate too little data to the validation set (e.g., less than 20%), your evaluation may not be very reliable, and the validation set's accuracy might not accurately predict how well your model will perform on unseen test data.\n"
      ],
      "metadata": {
        "id": "Ip_ivmV3GjYh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b675e5c4-ed7e-43f6-e629-688eec29f7d0"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Single Split: When you use just one split (train/validation/test), the results can be influenced by the specific data in that split. This means that your model's accuracy might vary depending on the luck of the draw when creating the split. It might not reflect the model's true performance on average.\n",
        "\n",
        "    Multiple Splits (Cross-Validation): Instead of relying on one split, you can perform multiple splits of your data into training and validation sets. Each time, you train your model on one split and validate it on another. By doing this multiple times (e.g., 5 or 10 splits), you get several accuracy scores.\n",
        "\n",
        "    Averaging: You can then calculate the average of these accuracy scores. This average provides a more stable and robust estimate of your model's performance because it accounts for variations in the data splits. It reduces the impact of random factors that might affect a single split's accuracy."
      ],
      "metadata": {
        "id": "a_TrWhfKHmzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using cross-validation, which involves averaging validation accuracy across multiple data splits, provides a more accurate estimate of test accuracy compared to a single validation split. However, it's important to clarify what we mean by \"test accuracy\" in this context.\n",
        "\n",
        "    Validation Accuracy: Validation accuracy is used to assess how well your model is likely to perform on unseen data (like the test set). When you perform cross-validation and average the validation accuracy scores over multiple splits, you get a more reliable estimate of how well your model generalizes to unseen data. This estimate is more accurate than relying on a single validation split.\n",
        "\n",
        "    Test Accuracy: Test accuracy, on the other hand, is the performance metric you compute on a completely separate dataset that the model has never seen before. This is the final evaluation to understand how well your model truly performs in a real-world scenario.\n",
        "\n",
        "While cross-validation gives you a better estimate of how your model will perform on unseen data, it doesn't replace the need for a dedicated test set evaluation. The test set accuracy remains the most accurate estimate of how your model will perform in practice."
      ],
      "metadata": {
        "id": "2HSkFUEoHnH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple words, the number of iterations can affect the estimate of your model's performance, but more iterations don't necessarily guarantee a better estimate.\n",
        "\n",
        "\n",
        "1. Fewer Iterations:\n",
        "   - If you use too few iterations (or epochs) when training your model, it might not have enough chances to learn from the data effectively.\n",
        "   - This can result in underfitting, where the model doesn't capture the data's patterns well, and the estimate of performance may be lower than what the model could achieve with more training.\n",
        "\n",
        "2. Adequate Iterations:\n",
        "   - There's usually a sweet spot where the model trains for an adequate number of iterations.\n",
        "   - In this range, the model learns well without overfitting (fitting too closely to the training data), and the estimate of performance is reliable.\n",
        "\n",
        "3. Too Many Iterations:\n",
        "   - Training for an excessive number of iterations can lead to overfitting, where the model starts to memorize the training data instead of learning the underlying patterns.\n",
        "   - In this case, the estimate of performance on validation data may seem high, but it won't reflect the model's actual ability to generalize to new, unseen data."
      ],
      "metadata": {
        "id": "GzRwLFaYHnWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Small Training Dataset:\n",
        "   - When your training dataset is very small, the model may struggle to learn and capture meaningful patterns in the data.\n",
        "   - Increasing the number of iterations can help the model to learn more from the limited data, but it can only do so much.\n",
        "   - There's a limit to how much improvement you can achieve with more iterations, and the model may still underperform compared to a larger dataset.\n",
        "\n",
        "2. Small Validation Dataset:\n",
        "   - If your validation dataset is very small, it can lead to less reliable estimates of your model's performance.\n",
        "   - Increasing the number of iterations won't necessarily solve this problem because the issue lies in the size of the validation data.\n",
        "   - A small validation dataset can lead to more variability in accuracy scores, and relying solely on iterations won't make these scores more trustworthy.\n",
        "\n",
        "In summary, while increasing the number of iterations can help a model learn better from small datasets, it cannot completely overcome the limitations of having very little data."
      ],
      "metadata": {
        "id": "PuPAjImmHnkk"
      }
    }
  ]
}